<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
  <style>
    .navA{
      display: inline-block;
      margin-right: 13px;
      font-size: 16px;
      font-weight: 700;
      color: #000;
      text-decoration: none;
      padding: 5px ;
      border: #000 1px solid;
    }
    .navA:hover{
      color: #fff;
      background-color: #000;
    }
  </style>

  <title>Efficient Deep Learning Reading Group</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/ys_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Efficient Deep Learning Reading Group <a href="https://zhuanlan.zhihu.com/p/601150600" target=_blank> (知乎link)</a></heading> </name>


              </p>

              <p>
                Are you interested in efficient deep learning but find it hard to keep up with the latest research? Join our Efficient Deep Learning Reading Group! Our group is focused on reading and discussing the most important and influential papers in deep learning, with a special emphasis on efficiency and practical applications.

                By joining our group, you'll have the opportunity to:
            </p>

            <ul>
                <li><p> Stay up-to-date with the latest research in efficient deep learning, without having to spend countless hours sifting through papers on your own.
                </p>
                <li><p> Engage in thoughtful and productive discussions with other deep learning enthusiasts, sharing your insights and learning from others.
                </p>
                <li><p> Develop a deeper understanding of the key concepts and techniques in deep learning, and how they can be applied in real-world scenarios.
                </p>
                <li><p> Connect with like-minded individuals and build meaningful relationships with friends who are also interested in related fields.
                </p>
            </ul>
            <p>
                <font color="#FF8166">Our group meets once a week via Zoom, and sessions typically run for 60 minutes every Sunday at 9PM (EST) since Feb 26, 2023</font>. We welcome participants of all backgrounds and experience levels, as long as you have a basic understanding of deep learning fundamentals. To ensure a high-quality experience for all members, we do ask that you commit to attending regularly and actively participating in discussions.

                If you're interested in joining our group, please fill out the application form inside <a href="https://zhuanlan.zhihu.com/p/601150600" target=_blank> (知乎link)</a>. We look forward to hearing from you!
            </p>

              <p>
                  Organizer:<br>
                <b><a href="https://eclipsess.github.io/yangsui.github.io/" target=_blank>Yang Sui</a></b>
              </p>

              <p>
                Members:<br>
<!--                  <b><a href="https://eclipsess.github.io/yangsui.github.io/" target=_blank>Yang Sui</a></b>,-->
                  <b><a href="" target=_blank>Wenjin Zhang</a></b><br>
                  <b><a href="" target=_blank>Qizhen Ding</a></b><br>
                  <b><a href="https://scholar.google.com/citations?user=ibPjG1YAAAAJ&hl=en" target=_blank>Jiechao Gao</a></b><br>
                  <b><a href="" target=_blank>Ye Tao</a></b><br>
                  <b><a href="" target=_blank>Lujun Li</a></b><br>
                  <b><a href="" target=_blank>Yang Zheng</a></b><br>
                  <b><a href="https://sites.google.com/site/douxiaotianjason/" target=_blank>Xiaotian Dou</a></b><br>
                  <b><a href="" target=_blank>Jianwei Li </a></b><br>
                  <b><a href="" target=_blank>Yu Wu</a></b><br>
                  <b><a href="" target=_blank>Xin Huang</a></b><br>
                  <b><a href="" target=_blank>Chengyuan Deng</a></b><br>
                  <b><a href="" target=_blank>Weizhao Jin</a></b><br>
                  <b><a href="" target=_blank>Xudong Wang</a></b><br>
                  <b><a href="" target=_blank>Zexing Xu</a></b><br>
                  <b><a href="" target=_blank>Yuhang Yao</a></b><br>
                  <b><a href="" target=_blank>Rulin Shao</a></b><br>
                  <b><a href="" target=_blank>Ruichao Li</a></b><br>
                  <b><a href="" target=_blank>Nianyi Wang</a></b><br>
                  <b><a href="" target=_blank>Xiang Pan</a></b><br>
                  <b><a href="https://github.com/ChulanZhang" target=_blank>Pengcheng Wang </a></b><br>
                  <b><a href="https://github.com/taokz" target=_blank>Kai Zhang</a></b><br>
                  <b><a href="" target=_blank>Bowen Lei</a></b><br>
                  <b><a href="https://scholar.google.com/citations?user=iBnBTukAAAAJ&hl=en" target=_blank>Dan Liu</a></b><br>
                  <b><a href="" target=_blank>Yanfeng Qu</a></b><br>
                  <b><a href="liuchen1993.cn" target=_blank>Chen Liu</a></b><br>
                  <b><a href="" target=_blank>Bin Hu</a></b><br>
                  <b><a href="http://huanwang.tech/" target=_blank>Huan Wang</a></b><br>
                  <b><a href="" target=_blank>Shan Xue</a></b><br>
                  <b><a href="http://qisunchn.top/" target=_blank>Qi Sun</a></b><br>
                  <b><a href="" target=_blank>Jufeng Guo</a></b><br>
                  <b><a href="" target=_blank>Yingcong Li</a></b><br>
                  <b><a href="chengyu-dong.me" target=_blank>Chengyu Dong</a></b><br>

              </p>

<!--              <p>-->
<!--                  Howdy! I'm a fourth year Ph.D. student at <a href="https://www.ece.rutgers.edu/"> Rutgers University, ECE Department</a>, working on deep learning, machine learning and computer vision, advised by <a href="https://sites.google.com/site/boyuaneecs/">Prof. Bo Yuan</a>. I received my M.S. and B.E. at <a href="https://ee.jlu.edu.cn/">Jilin University</a>.-->
<!--              </p>-->

<!--              <p>-->
<!--                I'm currently a Research Intern at <a href="https://multimedia.tencent.com/">Media Lab, Tencent America</a>, working with <a href="https://scholar.google.com/citations?user=6glC_iEAAAAJ&hl=en">Dr. Ding Ding</a>, <a href="https://scholar.google.com/citations?user=w_BcpK8AAAAJ&hl=en">Prof. Zhenzhong Chen</a> since 2022, exploring efficient neural image compression and Transformer models. In 2019, I was a full-time Algorithm Engineer at <a href="https://corporate.jd.com/">JD</a>, working on the face verification and recognition. I also spent a wonderful time as a Research and Development Intern, initializing the deep learning inference framework <a href="https://github.com/PaddlePaddle/Paddle-Lite">Paddle-Lite</a> (<font color="#FF8166">6.4k stars now</font>) at <a href="https://en.wikipedia.org/wiki/Baidu">Baidu</a> in 2018.-->
<!--              </p>-->

<!--              <p style="text-align:center">-->
<!--                <a href="mailto:ys764@scarletmail.rutgers.edu">Email</a> &nbsp/&nbsp-->
<!--                <a href="Yang_files/YangSui_CV.pdf">CV (Feb 2023)</a> &nbsp/&nbsp-->
<!--                <a href="https://scholar.google.com/citations?user=Q2W1p6sAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp-->
<!--                <a href="https://www.linkedin.com/in/yang-sui-308055117/">LinkedIn</a> &nbsp&nbsp-->
<!--              </p>-->


<!--            </td>-->
<!--            <td style="padding:2.5%;width:40%;max-width:40%">-->
<!--              <a href="images/yangsui_cv.png"><img style="width:80%;max-width:80%" alt="profile photo" src="images/yangsui_cv.png" class="hoverZoomLink"></a>-->
<!--            </td>-->
          </tr>
        </tbody></table>

        <div class="navbar" style="padding-left: 18px;">
          <a href="#Feb 2023" class="navA">Feb 2023</a>
          <a href="#Mar 2023" class="navA">Mar 2023</a>
        </div>

<!--        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--        <tr>-->

<!--          <td width="100%" valign="middle">-->

<!--            <heading id="Research"><b>Research</b></heading>-->
<!--&lt;!&ndash;             <p>-->
<!--            I am interested in efficient & reliable deep learning for AI at scale, investigating how to improve the efficiency of deep learning systems to achieve Pareto optimality between computing resources (e.g., parameter, data, computation) and model performance (e.g., inference, training). My long-term research goal is to free AI from the parameter-data-computation hungry beasts, and democratize AI to serve a broader area and population.-->
<!--            </p> &ndash;&gt;-->
<!--            <p>-->
<!--            I'm interested in efficient deep learning, investigating how to optimize computational resources (e.g., parameters, data, computation) and model performance (e.g., inference, training).-->
<!--            </p>-->

<!--            <ul>-->
<!--            <li><p>-->
<!--              Efficient Training & Model Compression Algorithms-->
<!--              &lt;!&ndash; Neural Architecture Search, Pruning, Knowledge Distillation &ndash;&gt;-->
<!--            </p>-->
<!--            <li><p>-->
<!--              Algorithm-hardware Co-design for AI Acceleration-->
<!--              &lt;!&ndash; DeepLearning-hardware Co-design, Reduced-cost Training, Trainingless Proxies &ndash;&gt;-->
<!--            </p>-->
<!--            <li><p>-->
<!--              Efficient Vision Transformer-->
<!--              &lt;!&ndash; DeepLearning-hardware Co-design, Reduced-cost Training, Trainingless Proxies &ndash;&gt;-->
<!--            </p>-->
<!--            <li><p>-->
<!--              Neural Image Compression-->
<!--            </p>-->
<!--            <li><p>-->
<!--              Error Correct Coding-->
<!--            </p>-->

<!--            </ul>-->
<!--          </td>-->
<!--        </tr>-->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading id="Feb 2023"><b>Feb 2023</b></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>02/26/2023</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="padding:20px;width:100%;vertical-align:middle"><tbody>

          <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/2201.13096" target=_blank>SPDY: Accurate Pruning with Speedup Guarantees</a></papertitle>, ICML'22.
              </a>
              <br>
              Elias Frantar, Dan Alistarh
<!--              <em><b>[ICML 2022]</b></em> <i>ICML 2022</i>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/2206.08684">PDF</a>-->
              <p><b>Presenter: <a href="https://eclipsess.github.io/yangsui.github.io/" target=_blank><font color="#FF8166">Yang Sui</font></a></b>
              <br>
              <a href="Slides/YangSui-EDLRG-022623.pdf" target=_blank>Slides</a>
              </p>

            </td>
          </tr>

          <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/2206.08684" target=_blank>Sparse Double Descent: Where Network Pruning Aggravates Overfitting</a></papertitle>, ICML'22.
              </a>
              <br>
              Zheng He, Zeke Xie, Quanzhi Zhu, Zengchang Qin
              <p><b>Presenter: <a href="https://eclipsess.github.io/yangsui.github.io/" target=_blank><font color="#FF8166">Yang Sui</font></a></b>
                            <br>
              <a href="Slides/YangSui-EDLRG-022623.pdf" target=_blank>Slides</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/2203.15794" target=_blank>CHEX: CHannel EXploration for CNN Model Compression</a></papertitle>, CVPR'22.
              </a>
              <br>
              Zejiang Hou, Minghai Qin, Fei Sun, Xiaolong Ma, Kun Yuan, Yi Xu, Yen-Kuang Chen, Rong Jin, Yuan Xie, Sun-Yuan Kung
              <p><b>Presenter: <a href="https://eclipsess.github.io/yangsui.github.io/" target=_blank><font color="#FF8166">Yang Sui</font></a></b>
                            <br>
              <a href="Slides/YangSui-EDLRG-022623.pdf" target=_blank>Slides</a>
              </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading id="Mar 2023"><b>Mar 2023</b></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>03/05/2023</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="padding:20px;width:100%;vertical-align:middle"><tbody>

          <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/2006.01862" target=_blank>Consistent Estimators for Learning to Defer to an Expert</a></papertitle>, ICML'20.
              </a>
              <br>
              Hussein Mozannar, David Sontag
              <p><b>Presenter: <a href="" target=_blank><font color="#FF8166">Yu Wu</font></a></b></p>
            </td>
          </tr>

          <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/2006.16669" target=_blank>EasyQuant: Post-training Quantization via Scale Optimization</a></papertitle>, arXiv'20.
              </a>
              <br>
              Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu, Debing Zhang
              <p><b>Presenter: <a href="" target=_blank><font color="#FF8166">Ruichao li</font></a></b></p>
            </td>
          </tr>

                  <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://sites.duke.edu/angli/files/2021/10/2021_Mobicom_Hermes_v1.pdf" target=_blank>Hermes: An Efficient Federated Learning Framework for Heterogeneous Mobile Clients</a></papertitle>, MobiCom'21.
              </a>
              <br>
              Ang Li, Jingwei Sun, Pengcheng Li, Yu Pu, Hai Li, Yiran Chen
              <p><b>Presenter: <a href="" target=_blank><font color="#FF8166">Bin Hu</font></a></b></p>
            </td>
          </tr>

                  <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/1906.02773" target=_blank>One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers</a></papertitle>, NeurIPS'19.
              </a>
              <br>
              Ari S. Morcos, Haonan Yu, Michela Paganini, Yuandong Tian
              <p><b>Presenter: <a href="" target=_blank><font color="#FF8166">Qizhen Ding</font></a></b></p>
            </td>
          </tr>

                  <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/2301.12900" target=_blank>DepGraph: Towards Any Structural Pruning</a></papertitle>, CVPR'23.
              </a>
              <br>
              Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, Xinchao Wang
              <p><b>Presenter: <a href="" target=_blank><font color="#FF8166">Chengyuan Deng</font></a></b></p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>03/12/2023</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="padding:20px;width:100%;vertical-align:middle"><tbody>


            <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/2208.11580" target=_blank>Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning</a></papertitle>, NeurIPS'22.
              </a>
              <br>
              Elias Frantar, Sidak Pal Singh, Dan Alistarh
              <p><b>Presenter: <a href="" target=_blank><font color="#FF8166">Jianwei Li</font></a></b></p>
            </td>
          </tr>

          <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="https://arxiv.org/abs/2206.14486" target=_blank>Beyond neural scaling laws: beating power law scaling via data pruning</a></papertitle>, NeurIPS'22.
              </a>
              <br>
              Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, Ari S. Morcos
              <p><b>Presenter: <a href="" target=_blank><font color="#FF8166">Wenjin Zhang</font></a></b></p>
            </td>
          </tr>


          <tr>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><a href="" target=_blank></a></papertitle>, NeurIPS'22.
              </a>
              <br>
              Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, Ari S. Morcos
              <p><b>Presenter: <a href="" target=_blank><font color="#FF8166">Junfeng Guo</font></a></b></p>
            </td>
          </tr>

        </tbody></table>









        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:0px">
              <br><hr width=80%>
              <p style="text-align:right;font-size:small;">
                *Last updated on 02/16/2023*
                <br>
                <a href="https://jonbarron.info/" target=_blank>Inspired by Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
